<html>
<head>
<title>e-Portfolio for Intelligent Agents course Summer 2024</title>
</head>

<body style="background-color: ffffbb; margin-left: 50px; margin-right: 50px;">
<h1>e-Portfolio for Intelligent Agents course Summer 2024</h1>
<h2>Chris Butterworth</h2>
<!-- <p><a href="about-me.html">About me</a></p> -->
<p><a href="https://butterworthc.github.io/ml-course/about-me.html">About me</a></p>
<p><a href="prep.html">Preparation for Intelligent Agents module</a></p>

<p>GitHub-hosted version: <a href="https://butterworthc.github.io/IntelligentAgents/e-Portfolio.html">butterworthc.github.io/IntelligentAgents/e-Portfolio.html</a></p>
<p>Online repository, showing commit descriptions: <a href="https://github.com/ButterworthC/IntelligentAgents">https://github.com/ButterworthC/IntelligentAgents</a></p>

<h3>Unit 1: Introduction to Agent-Based Computing</h3>
<p><b>Unit 1 Lecturecast</b></p>
<p>I had already read the first chapter of our textbook (<a href="#wooldridge">Wooldridge, 2009</a>) when I saw this, and noticed that it was
the source for most of the lecturecast material. However, the lecturecast was a more concise and memorable presentation of the material.</p>
<p><b>Unit 1 Readings</b></p>
<p><a href="#wooldridge">Wooldridge (2009)</a> Chapter 2: "Intelligent Agents" defines an agent as "a computer system that is <i>situated</i>
in some <i>environment</i>, and that is capable of <i>autonomous action</i> in this environment in order to meet its delegated objectives."
An ethical angle emerges on page 23 in a discussion of the factors which would trigger human intervention, one of which is:
"when the decision might cause harm." This reminded me of Asimov's Three Laws of Robotics, the first of which states:
"A robot may not injure a human being, or, through inaction, allow a human being to come to harm" (<a href="#asimov">Asimov, 1942</a>).
Wooldridge goes on to describe types of environments and agents and their capabilities, comparing agents with objects and expert systems.
Inputs, outputs and state are described, along with utility and the use of predicate functions.  It is a good, wide-ranging introduction.</p>

<h3>Unit 2: Introducing First Order Logic</h3>
<p>As described on my "preparation" page linked to above, I read about FOL in the two books shown, and watched a series of videos on the subject.
The concepts are not very difficult to understand but it can sometimes take a long time to turn a convoluted expression into an English sentence.
The biggest problem is the choice of symbols - some of the operators are in very obscure characters and 
some are not included with Microsoft Word&trade;.</p>
<p><b>Unit 2 Readings</b></p>
<p>The prescribed reading for this unit was <a href=#russell">Russell &amp; Norvig (2022)</a> Chapter 8: "First-Order Logic" 
but I found it useful to read Chapter 2: "Intelligent Agents" before this, as it had sections on rationality and
the structures of different types of agent, introducing representation and the relationships between entities.
I think I will be coming back to Chapter 8 quite often (and the sources mentioned above) to refresh my understanding of FOL in agents.</p>

<h3>Unit 3: Agent Architectures</h3>
<p><b>Unit 3 Lecturecast</b></p>
<p>This explains that agents consist of software and "architecture", of which it gives definitions from <a href="#maes">Maes (1991)</a> and 
<a href="#kaelbling">Kaelbling (1991)</a>, both of which are examined in the readings for this unit. There are sections on representation, 
reasoning and ontology, and an introduction to Brooks Behavioural Language.</p>
<p><b>Unit 3 Readings</b></p>
<p><a href="#maes">Maes (1991)</a> described systems of agents with different but interchangeable roles, 
with the ability to adjust their own importance to particular tasks.</p>
<p><a href="#kaelbling">Kaelbling (1991)</a> is about "the information content of the internal states of a machine."
One sentence that caught my eye on the first page was:
"the world may change during the computation, 
invalidating the initial semantic interpretation of the inputs, 
and possibly, therefore, the interpretation of the outputs."  So these agents need to be quick.  
Emphasis is put on the modular architecture and the incremental nature of software development.
It is made clear that this system is suitable for dynamic environments.
A distinction is made between dynamic data, which needs to be kept in individual machines,
and static data, which only needs to be in one instance, thereby reducing computing requirements.
Reference is made to <a href="#brooks">Brooks (1991)</a>, which is also in this week's reading list.</p>
<p><a href="#bratman">Bratman (1988)</a> discusses a combination of the weighing of alternatives, as in <a href="#maes">Maes (1991)</a>,
and "means-end reasoning," while bearing in mind "resource boundedness," which apparently means that agents:
"are unable to perform arbitrarily large computations in constant time."  
This recognises the problem I saw in <a href="#kaelbling">Kaelbling (1991)</a>, about getting out of synch with the state of environment,
although a footnote on page 5 says that applying these systems to real-time situations was a relatively recent idea.
The paper goes on to describe how a planning function could reduce the computing requirement by limiting the necessary reasoning.
It sounds to me analogous to the shortcutting of an AND statement when the first argument evaluates to FALSE and the second argument is not evaluated.
Also, knowledge is also bounded so that machines "are neither prescient nor omniscient."
This bounding takes the form of filtering, e.g. by time frame, location or compatibility with existing plans.</p>
<p><a href="#brooks">Brooks (1991)</a> got me in a tangle over the year to include in the citation. This paper was received by <i>Artificial Intelligence</i>
in 1987 but not printed for four years, hence the year 1991 is cited. Perhaps this means that many papers are actually older than they seem.
This paper is about agents ("independent and parallel activity producers") that interface to the environment more than to each other.
There is an interesting declaration on the first page: "No one talks about replicating the full gamut of human intelligence any more. 
Instead we see a retreat into specialized subproblems." 
That may have been true in 1987 but today all those elements are being brought together again (<a href="#goertzel">Goertzel &amp; Pennachin, 2007</a>).
Brooks advocates an incremental approach, starting with fairly simple systems. He is effectively saying we have to walk before we can run.
By "no representation" Brooks is saying a system can make decisions by 
"using the world as its own model and to continuously match the preconditions of each goal against the real world."
I found this paper to be perhaps more optimistic than realistic, but that may be because I do not fully understand the mechanisms at work.</p>
<p>Finally, <a href="#reynolds">Reynolds (1987)</a> discusses computer animation of a flock of birds, where 
"the aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds."
It is a similar problem to those in the first four papers. For these "boids" (simulated birds), three behaviours are identified: "collision avoidance..., 
velocity matching... and flock centering."  As with the other papers, computing is in LISP.</p>

<h3>Collaborative Discussion 1: Agent Based Systems</h3>
<p></p>

<h3>Unit 4: Hybrid Agent Architectures</h3>
<p><b>Unit 4 Reading</b></p>
<p>The prescribed passage was <a href="#wooldridge">Wooldridge (2009)</a> Section 5.2, titled "Hybrid Agents," but I read the whole of Chapter 5.
This included a section on reactive agents, and much material from the papers read in Unit 4 was summarised there. 
Hybrid agents combine at least two layers such as proactive and reasoning or planning layers, which can work either in parallel 
(where each is connected to sensor input and effector output) or series (where the inputs and outputs are connected to interface layers and the proactive 
and reasoning layers pass control to each other).  Other layers can include modelling layers, cooperation layers etc.
Four examples are given: TouringMachines, InterRRaP, 3T and Stanley, a self-driving Volkswagen.</p>

<h3>Unit 5: Agent Communication</h3>
<p><b>Unit 5 Lecturecast</b></p>
<p>This goes into the ontologies of agentsa, and their alignment.  It is based on the readings for this unit 
but also goes into knowledge query and manipulation language (KQML) and the knowledge interchange format (KIF).</p>

<p><b>Unit 5 Readings</b></p>
<p><a href="#searle">Searle (1969)</a> begins by asking simple questions about words and meaning, building up to the philosophy of language,
which he is keen to distance from "linguistic philosophy." 
He soon starts defining obscure terms: "Synonymy is defined as: two words are synonymous if and only if they have the same meaning; 
and analyticity is defined as: a statement is analytic if and only if it is true in virtue of its meaning or by definition."
Then he states the need for "some objective test for analyticity and synonymy."
Searle gives the impression of an eccentric with a lot of time on his hands, digging ever deeper into these concepts and the meaning of "meaning."
He even cites <a href="#wittgenstein">Wittgenstein (1953)</a> as pointing out that "exactness is relative to some purpose."
Searle seems to think that the inclusion of many examples will clarify the linguistic problems he describes but I think they obscure them.
By Chapter 2, Searle has formulated "that speaking a language is engaging in a rule-governed form of behavior..., performing acts according to rules"
and here he defines "different kinds of speech acts..., propositions, rules, meaning, and facts."
He explains that speech acts consist of utterance acts, propositional acts, illocutionary acts and perlocutionary acts.
I avoided learning his symbolism to prevent confusion with the symbolism of first order logic.
I was glad of the opportunity to learn new words such as <i>idiolect</i>, <i>illocution</i> and <i>perlocution</i>, 
but I found this to be a very turgid book and am pleased to see it does not reappear in the reading lists for future units in this module.</p>
<p><a href="#payne">Payne (2014)</a> discusses solutions to the problem of different agents having different ontologies and even vocabularies.
Possible solutions include giving agents access to a mapped list of equivalent terms 
and letting agents exchange messages until they develop some knowledge of their ontological differences.
The authors introduce their idea of "the Correspondence Inclusion Dialogue (CID), whereby agents negotiate by exchanging beliefs of the utilities 
of each correspondence."  
They are thus able to align their vocabularies by a process of accepting or rejecting matches, assigning to each pair a "degree of belief".
The paper is presented succinctly and is to the point.</p>

<h3>Unit 6: Working Together</h3>

<h3>Unit 7: Natural Language Processing (NLP)</h3>

<h3>Unit 8: Understanding Natural Language Processing (NLP)</h3>

<h3>Unit 9: Introduction to Adaptive Algorithms</h3>

<h3>Unit 10: Deep Learning in Action</h3>

<h3>Unit 11: Intelligent Agents in Action</h3>

<h3>Unit 12: The Future of Intelligent Agents</h3>

<h3>References</h3>
<p id="asimov">Asimov, I. (1942) Runaround. <i>Astounding Science Fiction</i> (March, 1942). New York: Street &amp; Smith Publications.</p>
<p id="bratman">Bratman, M.E., Israel, D.J. &amp; Pollack, M.E. (1988) Plans and resource-bounded practical reasoning. <i>Computational Intelligence</i> 4(3) 349-355. DOI: <a href="https://doi.org/10.1111/j.1467-8640.1988.tb00284.x">https://doi.org/10.1111/j.1467-8640.1988.tb00284.x</a></p>
<p id="brooks">Brooks, R. (1991) Intelligence without representation. <i>Artificial Intelligence</i> 47(1-3): 139-159. DOI: <a href="https://doi.org/10.1016/0004-3702(91)90053-M">https://doi.org/10.1016/0004-3702(91)90053-M</a></p>
<p id="cawsey">Cawsey, A. (1998) <i>The Essence of Artificial Intelligence</i>. Harlow: Pearson Education.</p>
<p id="gilbert">Gilbert, N. (2020) <i>Agent-Based Models</i>. 2nd ed. Thousand Oaks: SAGE Publications.</p>
<p id="goertzel">Goertzel, B., &amp; Pennachin, C. (Eds.). (2007). <i>Artificial General Intelligence</i>. Berlin: Springer.</p>
<p id="kaelbling">Kaelbling, L.P. (1991) A situated-automata approach to the design of embedded agents. <i>ACM SIGART Bulletin</i> 2(4): 85 - 88. DOI: <a href="https://doi.org/10.1145/122344.122361">https://doi.org/10.1145/122344.122361</a></p>
<p id="langton">Langton, C.G. (1995) <i>Artificial Life: An Overview</i>. Cambridge, MA: MIT Press.</p>
<p id="lee">Lee, Siu-Fan (2017) <i>Logic: A Complete Introduction</i>. London: Hodder Education.</p>
<p id="maes">Maes, P. (1991) The agent network architecture. <i>ACM SIGART Bulletin</i> 2(4): 115-120. DOI: <a href="https://doi.org/10.1145/122344.122367">https://doi.org/10.1145/122344.122367</a></p>
<p id="payne">Payne, T.R. &amp; Tamma, V. (2014) Negotiating over ontological correspondences with asymmetric and incomplete knowledge. <i>AAMAS '14: Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems</i> Sorbonne University, Paris, 5-9 May. 517-524.</p>
<p id="reynolds">Reynolds, C.W. (1987) Flocks, Herds, and Schools: A Distributed Behavioral Model. <i>Computer Graphics</i> 21(4): 25-34. DOI: <a href="https://doi.org/10.1145/37402.37406">https://doi.org/10.1145/37402.37406</a></p>
<p id="russell">Russell, S. &amp; Norvig, P. (2022) <i>Artificial Intelligence: A Modern Approach</i>. 4th ed. Harlow: Pearson Education.</p>
<p id="searle">Searle, J.R. (1969) <i>Speech Acts: An Essay in the Philosophy of Language</i>. Cambridge University Press. DOI: <a href="https://doi.org/10.1017/CBO9781139173438">https://doi.org/10.1017/CBO9781139173438</a></p>
<p id="testfatsion">Testfatsion, L &amp; Judd, K. (2006) ‘Handbook of Computational Economics, Vol. 2: Agent-Based Computational Economics’, in: Amman, H.M, Kendrick, D.A &amp; Rust, J. (eds) <i>Handbook of Computational Economics</i>. Amsterdam: Elsevier.</p>
<p id="viljoen">Viljoen, G. (2024) Initial Post to Collaborative Discussion 1: Agent Based Systems. Available from: <a href="https://www.my-course.co.uk/mod/forum/discuss.php?d=245817">www.my-course.co.uk/mod/forum/discuss.php?d=245817</a> [Accessed 13 August 2024].</p>
<p id="wittgenstein">Wittgenstein, L. (1953) <i>Philosophical Investigations</i>. Oxford: Blackwell.</p>
<p id="wooldridge">Wooldridge, M.J. (2009) <i>An Introduction to Multiagent Systems</i>. Chichester: John Wiley &amp; Sons.</p>
<p>&nbsp;</p>
<p>Microsoft Word&trade; is a registered trademark of Microsoft Corporation.</p>
<p>&nbsp;</p>
</body>
</html>